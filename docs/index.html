<html>
<head>
<link rel="shortcut icon" href="data:image/x-icon;," type="image/x-icon">
<script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@0.12.5"></script>
<style>
canvas{background-color:#ccc;border:1px solid #ccc;margin:8px}
body{font-family:Verdana}
#webcamElement{height:224px;margin:8px}
.example{width:224px;margin:2px}
</style>
</head>
<body>
<select onchange='refreshModel(this.value)'>
  <option value="headshoulderdata">headshoulderdata</option>
  <option value="humanparsingdata">humanparsingdata</option>
  <option value="persondata">persondata</option>
</select>
<button onclick='processing=!processing;processWebCam()'>webcam</button
Click on a test image to see the result of the segmentation by UNet<BR>
You can also <b>drag and drop</b> an image file to process it<BR>
<video width="224" height="224" autoplay="true" id="webcamElement"></video>
<canvas id="canvasElement" width="224" height="224"></canvas>
<canvas id="canvasElement2" width="224" height="224"></canvas>
<div id="imagecontainer"></div>
<script>
//http-server . -S -C ~/git.fxpal.net/docuchat/server.crt -K ~/git.fxpal.net/docuchat/server.key
// convert animated gif to mp4
// ffmpeg -i ~/Dropbox/fitmongo_appengine/static/images/headshot.gif -movflags faststart -pix_fmt yuv420p -vf "scale=trunc(iw/2)*2:trunc(ih/2)*2" ~/examples/Unet-for-Person-Segmentation/lolo/headshot.mp4
var unet = null;
var modelName = 'headshoulderdata';

const imw = 224;
const imh = 224;
const images = `00010.jpg
00011.jpg
00012.jpg
00013.jpg
00014.jpg
laurent_denoue.jpg`.split('\n');
async function loadUNet() {
	console.log('loading model...');
  const unet = await tf.loadModel(modelName + '/model.json');
  console.log('done.');
  console.log('warming up model...');
  unet.predict(tf.zeros([1, imh, imw, 3])).dispose();
  console.log('done.');
  return unet;
}

function refreshModel(newmodelname)
{
  modelName = newmodelname;
  init();
}

function adjustVideoSize(width, height) {
	const aspectRatio = width / height;
	console.log(aspectRatio);
	if (width >= height) {
		webcamElement.width = aspectRatio * webcamElement.height;
	} else if (width < height) {
		webcamElement.height = webcamElement.width / aspectRatio;
	}
}

async function setupVideo() {
	return new Promise((resolve, reject) => {
		webcamElement.addEventListener('loadeddata', async () => {
			this.adjustVideoSize(
					webcamElement.videoWidth,
					webcamElement.videoHeight);
			resolve();
		}, false);
		webcamElement.src = 'two.mp4';//'headshot.mp4';

	});
}

async function setupWebcam() {
	return new Promise((resolve, reject) => {
		const navigatorAny = navigator;
		navigator.getUserMedia = navigator.getUserMedia ||
				navigatorAny.webkitGetUserMedia || navigatorAny.mozGetUserMedia ||
				navigatorAny.msGetUserMedia;
		if (navigator.getUserMedia) {
			navigator.getUserMedia(
					{video: true},
					stream => {
						webcamElement.srcObject = stream;
						webcamElement.addEventListener('loadeddata', async () => {
							this.adjustVideoSize(
									webcamElement.videoWidth,
									webcamElement.videoHeight);
							resolve();
						}, false);
					},
					error => {
					  resolve();
						alert('no webcam');
					});
		} else {
			reject();
		}
	});
}

function cropImage(img) {
	const size = Math.min(img.shape[0], img.shape[1]);
	const centerHeight = img.shape[0] / 2;
	const beginHeight = centerHeight - (imh / 2);
	const centerWidth = img.shape[1] / 2;
	const beginWidth = centerWidth - (imw / 2);
	return img.slice([beginHeight, beginWidth, 0], [imh, imw, 3]);
}

var imgdata = canvasElement.getContext('2d').createImageData(imw,imh);
var data = imgdata.data;

function drawResult(l) {
	//console.log(l);
	var idx = 0;
	var idx4 = 0;
	for (var i=0;i<imh*imw;i++)
	{
		data[idx4] = 0;
		data[idx4+1] = 0;
		data[idx4+2] = 0;
		data[idx4+3] = 255;
		idx4 += 4;
	}
	idx4 = 0;
	for (var j=0;j<imh;j++)
	{
		for (var i=0;i<imw;i++)
		{
			var pixel = l[idx];
			var val = 0;
			if (pixel >= 0.8)
				val = 255;
			data[idx4] += val;
			data[idx4+1] += val;
			data[idx4+2] += val;
			data[idx4+3] = 255;
			idx++;
			idx4+=4;
		}
	}
	canvasElement2.getContext('2d').putImageData(imgdata,0,0);
}

var result = null;
async function init() {
	imagecontainer.textContent = 'loading model...';
  await setupWebcam();
  //await setupVideo();
  unet = await loadUNet();
  imagecontainer.textContent = 'loaded!';
	//print(unet.summary());
  // Warm up the model. This uploads weights to the GPU and compiles the WebGL
  // programs so the first time we collect data from the webcam it will be
  // quick.
  tf.tidy(() => {
  });
  
  imagecontainer.innerHTML = '';
  for (var i=0;i<images.length;i++)
  {
  	var img = document.createElement('img');
  	img.src = images[i];
		img.className = 'example';
  	img.title = images[i];
  	img.onclick = function () {
  		processing = false;
  		processImage(this);
  	};
  	imagecontainer.appendChild(img);
  }
  
}

var processing = false;
function processWebCam() {
	if (!processing)
		return;
	var w = webcamElement.videoWidth;
	var h = webcamElement.videoHeight
	canvasElement.getContext('2d').drawImage(webcamElement,0,0,w,h,0,0,imw,imh);
	var tfImage = tf.fromPixels(canvasElement);
	tfImage = tfImage.slice([0, 0, 0], [imh, imw, 3]);
	var batchedImage = tfImage.expandDims(0);
	batchedImage = batchedImage.toFloat().div(tf.scalar(255));
	//console.time('predict');
	result = unet.predict(batchedImage);
	//console.timeEnd('predict');
	//result = result.argMax(axis=2);
	//result = result.reshape([imh,imw]);
	//console.log(result.shape);
	//result.reshape
	result.data().then(l => drawResult(l));
	//console.log(result);
	//requestAnimationFrame(processWebCam);
	setTimeout(processWebCam,100);
}

function processImage(image) {
	var w = image.naturalWidth;
	var h = image.naturalHeight;
	canvasElement.getContext('2d').drawImage(image,0,0,w,h,0,0,imw,imh);
	var tfImage = tf.fromPixels(canvasElement);
	tfImage = tfImage.slice([0, 0, 0], [imh, imw, 3]);
	var batchedImage = tfImage.expandDims(0);
	batchedImage = batchedImage.toFloat().div(tf.scalar(255));
	//console.time('predict');
	result = unet.predict(batchedImage);
	//console.timeEnd('predict');
	//result = result.argMax(axis=2);
	//result = result.reshape([imh,imw]);
	result.data().then(l => drawResult(l));
}

function addImage(file) {
	var r = new FileReader();
	r.onload = function(e) { 
		var img = new Image();
		img.onload = function () {
			processing = false;
			processImage(this);
		};
		img.onerror = function(e) { console.error(e)};
		img.src = e.target.result;
	}
	r.readAsDataURL(file);
}

function handleData(dataTransfer)
{
	var files = dataTransfer.files;
	for (var i = 0; i < files.length; i++) {
		addImage(files[i]);
	}
}

function setDropZone(elem)
{
  elem.addEventListener('dragenter', function(evt) {
    evt.preventDefault();
    document.body.style.backgroundColor = '#ccc';
  }, false);
  elem.addEventListener('dragover', function(evt) {
    evt.preventDefault();
  }, false);
  elem.addEventListener('dragleave', function(evt) {
    evt.preventDefault();
    document.body.style.backgroundColor = 'white';
  }, false);
  elem.addEventListener('drop', function(evt) {
    evt.preventDefault();
    document.body.style.backgroundColor = 'white';
    console.log('drop');
    handleData(evt.dataTransfer);
  },false);
}

setDropZone(document.body);
init();

</script>

</body>
</html>

